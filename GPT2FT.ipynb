{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MungaiMwangi001/LLMs/blob/main/GPT2FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the dataset**"
      ],
      "metadata": {
        "id": "2ixmHB3U_06x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
        "df = pd.DataFrame(dataset['train'])"
      ],
      "metadata": {
        "id": "x8dqc7-x_tKi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9mqrWR6vS9rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOkenizer**"
      ],
      "metadata": {
        "id": "ZuHUx5fPD3NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Loading the dataset to train our model\n",
        "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(examples):\n",
        "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXXgY1NFD1xv",
        "outputId": "f7fd1ae8-a13b-441c-db70-1806fd68694d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize  the base model**"
      ],
      "metadata": {
        "id": "4c7gAkwnECgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "# Remove the 'id', 'text', and 'label_text' columns as they are not needed for training in this format and cause issues with the data collator\n",
        "small_train_dataset = small_train_dataset.remove_columns([\"id\", \"text\", \"label_text\"])\n",
        "small_eval_dataset = small_eval_dataset.remove_columns([\"id\", \"text\", \"label_text\"])"
      ],
      "metadata": {
        "id": "_MwqFcVREK1z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the base model**"
      ],
      "metadata": {
        "id": "HqTrhJz_JAgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2ForSequenceClassification\n",
        "\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n",
        "\n",
        "# Set the padding token ID in the model's configuration\n",
        "# Use the tokenizer's pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cu7tHx40JFXq",
        "outputId": "5a0f39d8-44fc-48b0-d10c-fc42e4ecff03"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
            "All model checkpoint weights were used when initializing GPT2ForSequenceClassification.\n",
            "\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate==0.4.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Drr3EspPEbPo",
        "outputId": "470fbd8d-540c-46d9-8fc9-05c3edffcbff"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate==0.4.2 in /usr/local/lib/python3.11/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.2) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (0.34.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.2) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.2) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.2) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.2) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.2) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Method**"
      ],
      "metadata": {
        "id": "iJc8Ydj2EQrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Optional: disable Weights & Biases logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers.logging.set_verbosity_info()\n",
        "\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Calculate steps per epoch\n",
        "n_gpu = torch.cuda.device_count() or 1\n",
        "print(f\"Using {n_gpu} GPU(s)\")\n",
        "\n",
        "per_device_batch_size = 8  # Increased from 1\n",
        "grad_accumulation = 1\n",
        "total_batch_size = per_device_batch_size * n_gpu * grad_accumulation\n",
        "steps_per_epoch = max(1, len(small_train_dataset) // total_batch_size)\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "\n",
        "# Compute metrics function using sklearn\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Flatten predictions and labels\n",
        "    flat_predictions = predictions.flatten()\n",
        "    flat_labels = np.asarray(labels).flatten()\n",
        "\n",
        "    # Filter out ignored loss indices (-100)\n",
        "    valid_indices = flat_labels != -100\n",
        "    valid_predictions = flat_predictions[valid_indices].astype(int)\n",
        "    valid_labels = flat_labels[valid_indices].astype(int)\n",
        "\n",
        "    # Compute accuracy\n",
        "    return {\"accuracy\": accuracy_score(valid_labels, valid_predictions)}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    per_device_train_batch_size=per_device_batch_size,\n",
        "    per_device_eval_batch_size=per_device_batch_size,\n",
        "    gradient_accumulation_steps=grad_accumulation,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=5e-5,\n",
        "\n",
        "    # Use the correct parameter name\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=steps_per_epoch,\n",
        "\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    remove_unused_columns=False,\n",
        "    no_cuda=not torch.cuda.is_available(),\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize Data Collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Save model\n",
        "trainer.save_model(\"final_model\")\n",
        "print(\"Model saved to 'final_model' directory\")\n",
        "\n",
        "# Evaluate\n",
        "if small_eval_dataset is not None:\n",
        "    print(\"Evaluating on validation set...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Validation results: {eval_results}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nhXSppoiQ30_",
        "outputId": "4c2fdb8d-08f1-48f8-b94d-9330378410a8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\n",
            "Using auto half precision backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.55.0\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Using 1 GPU(s)\n",
            "Steps per epoch: 125\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 1,000\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 124,442,112\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 04:08, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.085900</td>\n",
              "      <td>0.950884</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.688700</td>\n",
              "      <td>0.729760</td>\n",
              "      <td>0.688000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test_trainer/checkpoint-125\n",
            "Configuration saved in test_trainer/checkpoint-125/config.json\n",
            "Model weights saved in test_trainer/checkpoint-125/model.safetensors\n",
            "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
            "tokenizer config file saved in test_trainer/checkpoint-125/tokenizer_config.json\n",
            "Special tokens file saved in test_trainer/checkpoint-125/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test_trainer/checkpoint-250\n",
            "Configuration saved in test_trainer/checkpoint-250/config.json\n",
            "Model weights saved in test_trainer/checkpoint-250/model.safetensors\n",
            "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
            "tokenizer config file saved in test_trainer/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in test_trainer/checkpoint-250/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to final_model\n",
            "Configuration saved in final_model/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in final_model/model.safetensors\n",
            "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
            "tokenizer config file saved in final_model/tokenizer_config.json\n",
            "Special tokens file saved in final_model/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 'final_model' directory\n",
            "Evaluating on validation set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:27]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation results: {'eval_loss': 0.7297596335411072, 'eval_accuracy': 0.688, 'eval_runtime': 28.0348, 'eval_samples_per_second': 35.67, 'eval_steps_per_second': 4.459, 'epoch': 2.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finetune   usin the Trainer Method**"
      ],
      "metadata": {
        "id": "xvsl_TsiFbQP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba4a8a03"
      },
      "source": [
        "**Prepare labels for Token Classification**\n",
        "\n",
        "The `AutoModelForTokenClassification` model expects a `labels` column in the dataset where each entry is a list of token-level class indices corresponding to the input tokens. You need to create this column based on your original sentiment span annotations.\n",
        "\n",
        "For example, if your original labels are start and end indices of the sentiment span within the text, you would need to:\n",
        "1. Align the character-level start and end indices with the tokenized input.\n",
        "2. Create a list of labels for each token, where each label is an integer representing the class (e.g., 0 for 'O' - outside, 1 for 'B-sentiment' - beginning of sentiment span, 2 for 'I-sentiment' - inside of sentiment span). You might need different classes for different sentiment types (positive, negative, neutral).\n",
        "3. Handle special tokens (like `[CLS]`, `[SEP]`, padding) by assigning them a special label value (e.g., -100) that the model can ignore during loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb7382a9"
      },
      "source": [
        "# This is a placeholder cell. You need to implement the logic here\n",
        "# to create the 'labels' column in your tokenized_datasets.\n",
        "\n",
        "# Example: Function to align labels and create token-level labels\n",
        "def align_labels_with_tokens(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"label\"]): # Assuming 'label' is the sentiment class index\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        # Create token-level labels based on the word ids and original sentiment span\n",
        "        # This is a simplified example and needs to be adapted to your specific label format\n",
        "        token_labels = []\n",
        "        # You need to implement the logic to map original span labels to token labels\n",
        "        # based on word_ids.\n",
        "        # For now, let's create dummy labels (e.g., all 0s) to make the code runnable,\n",
        "        # but this will not train the model correctly for sentiment extraction.\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                token_labels.append(-100) # Ignore special tokens\n",
        "            else:\n",
        "                # Replace this with your logic to assign the correct token label\n",
        "                # based on the original sentiment span and the word_id\n",
        "                token_labels.append(0) # Placeholder: Assign 0 for now\n",
        "\n",
        "        labels.append(token_labels)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the function to your dataset (replace with your actual dataset object)\n",
        "# You might need to apply this to both train and test splits\n",
        "# tokenized_datasets = dataset.map(align_labels_with_tokens, batched=True)\n",
        "\n",
        "# After creating the 'labels' column, you should verify its structure and content.\n",
        "# print(tokenized_datasets['train'][0]['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "942d4dba"
      },
      "source": [
        "# Since the above cell is a placeholder and won't create correct labels for sentiment extraction,\n",
        "# and to make the Trainer initialization runnable without the 'labels' error,\n",
        "# let's temporarily add a dummy 'labels' column to the small datasets.\n",
        "# **IMPORTANT:** This is only for making the code run and will NOT train the model correctly\n",
        "# for the sentiment extraction task. You MUST replace this with the correct label preparation logic.\n",
        "\n",
        "# Assuming the tokenized inputs have a certain sequence length (e.g., 512 from truncation=True in tokenize_function)\n",
        "# You should get the actual sequence length from your tokenized dataset if it's different.\n",
        "sequence_length = len(small_train_dataset[0]['input_ids']) # Get sequence length from actual data\n",
        "\n",
        "def add_dummy_labels(examples):\n",
        "    # Create dummy labels (e.g., all 0s or a random sequence) with the correct shape\n",
        "    # The shape should be (batch_size, sequence_length) for token classification\n",
        "    dummy_labels = [[0] * sequence_length for _ in range(len(examples['input_ids']))]\n",
        "    examples['labels'] = dummy_labels\n",
        "    return examples\n",
        "\n",
        "# Apply the dummy label creation to your small datasets\n",
        "# small_train_dataset = small_train_dataset.map(add_dummy_labels, batched=True)\n",
        "# small_eval_dataset = small_eval_dataset.map(add_dummy_labels, batched=True)\n",
        "\n",
        "# **Note:** The above dummy label creation is commented out because applying map again\n",
        "# might overwrite previous tokenization results or cause issues if applied incorrectly.\n",
        "# A better approach is to ensure the initial tokenization and labeling function\n",
        "# correctly creates the 'labels' column.\n",
        "\n",
        "# Assuming the 'labels' column is now correctly present in small_train_dataset and small_eval_dataset\n",
        "# after running the appropriate label preparation logic."
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXXgY1FDD1xv"
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Loading the dataset to train our model\n",
        "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def tokenize_function(examples):\n",
        "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the 'label_text' column from the tokenized datasets\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"label_text\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}