{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MungaiMwangi001/LLMs/blob/main/GPT2FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ixmHB3U_06x"
   },
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VFAlu1MK_vY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "6ddedbdd59f24858a6b677647affc0dd",
      "159d34ccfb3a4a7e9be60cd085f5c772",
      "7ae9e53352964bcaa69fd39e736bf740",
      "5f4555d86db14639a572ea3e90e053e0",
      "cda566f5312f4b5aadf855f97c370dad",
      "2567f37cccb649c7a6f0ca503ee3f7ae",
      "f5e3300000fd448ba4d8c1d614c69d61",
      "7cd45c7a9a9c4dd597e5e9a52db48c21",
      "076e664b0dec4678b71cb5f92b10c3b9",
      "419a3f5557fa45ed86c88429fabc1de7",
      "525b129e56114ef28ac5c77cf314f95f",
      "800887edc3d046d38fd2e203f1b2842f",
      "b2cf5a6c76bc42e19b2dd9f329d0f829",
      "a01adf99ab8f4419908461196f1b44bb",
      "55b71caf8ea34dd8b7724c8869471202",
      "aa6fbdb7efdd40a3a512d50b3f1065ba",
      "edeb4645ce594ad8af04d6ae90a1d394",
      "7af7d234e40442699b3badea738e3d0f",
      "c8874e607f3d4178ac6648ddd085ebbb",
      "a8b4a1b2fbed40c4849c53beb6d22495",
      "f5e8117045df4075a79b1060504bf278",
      "030a0f8794c44710ad2f41b442bbbc51",
      "ba1ba6c83bef444386be1c9bf1023f22",
      "307a91df62cb4d27bbb0583ed593d0dc",
      "1f234e695c104b6f8753cc5ee6ffdf2b",
      "6ed26f1587aa4670acb18bc8c1d84632",
      "0e3a58e73452469c954c2cdbd8bf7001",
      "3e8cde4e02554b58a80e83cd064dd9e3",
      "4bfc6f90613640378f4aa7dd71f0f2a5",
      "609efa9580de4c03b71a0f6e87fb0220",
      "1d064338d8294dd7925978769479b098",
      "a997a4e406054b39bf1772739d463700",
      "27226da00d5245f6ac177ba05b579573",
      "1c700398539740f083f6aae283100ba4",
      "7661773842e04aaea78a9c9ab9f6cb9a",
      "fa75e31730104cafbd6121173385ea05",
      "79fb24e37763490db0d64a857dc9cd3d",
      "8505a04e3a104bbb8b73e70cda89a88a",
      "e069349b103e47448ee14a1a8673a6e7",
      "c38f10c541db4ee6bc2a236b4486a643",
      "c7e18b155adb4e6da058d6e099761102",
      "538b3ba2408e4de29c26e16e7d02a60f",
      "e9927befcd244bd1b5e81e0c63ea5b26",
      "1b99c94b839f4d00a3f1ee519b3d42f0",
      "a610344f1ca245aa9fb2423fde1b9260",
      "7ef3b3b752fd4ba987645561848bc80d",
      "a407e30f36b94b98b2a75681c86856e3",
      "7e98b2bb8bda4e0eb28e75b340286f42",
      "64d27107bb6c41f9a3e5a859bd98d920",
      "082d37816fde47cfa06127794d89da7d",
      "babfd7d97f20432197448eb6ad0fd847",
      "c7132dcf607f4f789bb98d053e44cc64",
      "471d8bf65b0644639fd39205e9a06bbf",
      "b7be30ab960e46cfa343c5344967ccfc",
      "a1f0bb1c8e0b4568b0b0e7b0486f6789"
     ]
    },
    "id": "x8dqc7-x_tKi",
    "outputId": "a484938f-6abf-450b-dc39-24f83988b1fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddedbdd59f24858a6b677647affc0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800887edc3d046d38fd2e203f1b2842f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/1.86M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1ba6c83bef444386be1c9bf1023f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/240k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c700398539740f083f6aae283100ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/26732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a610344f1ca245aa9fb2423fde1b9260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3432 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mqrWR6vS9rr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuHUx5fPD3NA"
   },
   "source": [
    "**TOkenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5ba19ab9352f46f385f9793fc80b7a43",
      "f756a39a18c84a7a8b6340ca4dd56d0d",
      "9d3c1630550643f48101676f782e2174",
      "c4184a5cfc2346959e49b3d679bb36f6",
      "c0d5a818c5bf4ed89f3335111afa5064",
      "5c98b5a1784943fa9ec50c20c42c9e19",
      "e206366810ae44acafef7af306948622",
      "04b86d5362c9400290fef2fe73ac6851",
      "44fdd4943db84b2c87de6ffa1e7b49d9",
      "f1faa500443c4193a1c1bee6c31aca66",
      "8a7b4f075c64447fad2d91d0848bcc71",
      "d241f4304efa4dd8a541656a934c7bf3",
      "b918138c01fa476ca19f8bdd5cad9fa4",
      "3302ea5a9b44414f85f3a9965916ca2e",
      "d27e6f1f5c234812aa37d55093c7bc2a",
      "4fda10da6efa4177b9ad9ff504b36828",
      "bb5be3989a664e71aa59ea50e29b5cbc",
      "4a3dcbb600cc45c2aa5ef45af8e6f3bf",
      "4cb2784ba7934fe6b91b7345f69052b4",
      "6ba63569f46a480b86e1f987ea24c29b",
      "3963d3afdcf044cea9066a7f650b60ae",
      "d80dc774a9584e4eb4c72a1ce14c4d97",
      "bd27c164bb4b4897ae7681ee7b457a4b",
      "4e92c7f8e4a141259f2ef91240b83aa9",
      "429a1aaee4b840f58de613d18c506aad",
      "3190ad91b0d64a85b2ce73bbf41e4f49",
      "893413112adf47118efb7dea8f75300b",
      "4e9fa7f3d5344e10bd97d1f8128d7bb3",
      "15bc97a093c345c5a932460af6fafe45",
      "93bc38c0e3634d359f3f531e2a30ccca",
      "4b18006b30354819bff3150ae869fc25",
      "7d247ad6722b4c408a8cca91437089d0",
      "27ee49a8843049e58ec4135c58c8fe4b",
      "d348911629bb42fdba9b171abbb16538",
      "a399c5fea95c41398333b77865a0ef77",
      "40f24d53edac4f5caefdf8e8c94f7fd0",
      "e97ba3ece1da47f0bfd0442c2649a434",
      "859567262adb42b892a02dd1e2895ef2",
      "ebbea336bef14ee7a66342dcc519b192",
      "23d3ef3d9e4f42558b304d5cf05ae4fb",
      "eff22d7b85f5431389530931ecd552d8",
      "c79f8cc71b1c4672b98701c438e6c38c",
      "3775b493d07740dcb64f310ba6387c75",
      "bab989d05b06492f9bc592612ca5f1b5",
      "f814ed8031e04cc2bda88edf37cbfef8",
      "ba16b88192404c6daffb75093707278e",
      "96e104fffae94f06a542c4a728de3d50",
      "fbe0d3cdbf8b46fcb43d01b3ad65d7eb",
      "bad8978345544370bb8e7039f4d66768",
      "e1c606831c5c4c48a408d52c6324646f",
      "06b33af14b9a409f8bbf68a201c4e6dd",
      "059e3f175e7d4fa3834aeaa307203485",
      "42607e742eb447beabb549a6a35d0f6c",
      "2284a327833944928bc01bdec4cb847d",
      "ed520bb3718d4f58b6d42c70d2f4c59c",
      "d80eb1a11b244c98b1fa74fb65817a55",
      "b4e595b7c21f4b9c9157681214416ede",
      "dd0a008382d749a98b7b07244053bfb8",
      "ef5f7625ee0642648d3b43a2861a96b0",
      "306ac588548c4c9fb207773491415d73",
      "18d0f1f5189d42aca0d06343b68edfc7",
      "3cb743c6bd644e2693bed7ba16949d7d",
      "0653a78369aa4564924eb716c5d7cfb7",
      "90f264bf66b94fb29f679c1f22619495",
      "e1f2820c33bb4ddab47670a86f921695",
      "bad4c60444a842a0b236abce5c26cac9",
      "8ed0caffd3e74ba596aafb726f3b4764",
      "47b9606f0bed4d54b2a9b61e4062a3bf",
      "7df8b3f49bd843e5be9b139dbe5f5172",
      "5b7a20b72fd341cab45b4a88e064c3d8",
      "0b0dec1c6e794b87a84cb035bac8bcc1",
      "704498b994284c23a2e46f11801f25c5",
      "e7c63b763a5c4d43a9fd3de52aa7de82",
      "8087df556226417a9033ddeaa5f08eae",
      "71fd783691e44883b27a07c76db7771e",
      "22314882b3c744788aee712c1e1db1ed",
      "b5bbe9bec509467794f1bdc237a9476d"
     ]
    },
    "collapsed": true,
    "id": "HXXgY1NFD1xv",
    "outputId": "41d001e9-b9b0-411a-a99f-f2a4a361b18d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba19ab9352f46f385f9793fc80b7a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d241f4304efa4dd8a541656a934c7bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd27c164bb4b4897ae7681ee7b457a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d348911629bb42fdba9b171abbb16538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f814ed8031e04cc2bda88edf37cbfef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.55.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80eb1a11b244c98b1fa74fb65817a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed0caffd3e74ba596aafb726f3b4764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3432 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Loading the dataset to train our model\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c7gAkwnECgv"
   },
   "source": [
    "**Initialize  the base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MwqFcVREK1z"
   },
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Remove the 'id', 'text', and 'label_text' columns as they are not needed for training in this format and cause issues with the data collator\n",
    "small_train_dataset = small_train_dataset.remove_columns([\"id\", \"text\", \"label_text\"])\n",
    "small_eval_dataset = small_eval_dataset.remove_columns([\"id\", \"text\", \"label_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqTrhJz_JAgP"
   },
   "source": [
    "**Initialize the base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ff5e360424c745af80d7b1f646759124",
      "a4c1f2cf8ca140d1a493f04b6c757a25",
      "a26ae10584da48ba89670595e80b54b0",
      "f3a06feba2f0481cabae9fa186d8ac3a",
      "b8eafcefe5f24b11acfeb9fea2871553",
      "9ac72d6c267c4457954a1ead20f2bac1",
      "a3b9fa5621444141830b806811128ff8",
      "213befc5742742f1b20c85c4d7fd796f",
      "18bd539c900944848ed78538cea68c86",
      "6f70fbbe741f407bab486eb571644daa",
      "565784699ff84c44a3b725934adef2e5"
     ]
    },
    "collapsed": true,
    "id": "cu7tHx40JFXq",
    "outputId": "a67de0a0-9be2-402d-b76b-f8e65f2c83c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.55.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5e360424c745af80d7b1f646759124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "All model checkpoint weights were used when initializing GPT2ForSequenceClassification.\n",
      "\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n",
    "\n",
    "# Set the padding token ID in the model's configuration\n",
    "# Use the tokenizer's pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Drr3EspPEbPo",
    "outputId": "46b3b9a7-8561-4249-a343-17d3152b30dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate==0.4.2\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.2) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (0.34.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.2) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (18.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.2) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.2) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.2) (1.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.2) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate==0.4.2) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.2) (1.17.0)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate==0.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJc8Ydj2EQrt"
   },
   "source": [
    "**Evaluate Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nhXSppoiQ30_",
    "outputId": "ef08f75d-b7c8-4b57-9e9e-12bec730ad46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.55.0\n",
      "PyTorch version: 2.6.0+cu124\n",
      "Using 1 GPU(s)\n",
      "Steps per epoch: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 1,000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 250\n",
      "  Number of trainable parameters = 124,442,112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.178200</td>\n",
       "      <td>1.046094</td>\n",
       "      <td>0.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>0.870866</td>\n",
       "      <td>0.643000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-125\n",
      "Configuration saved in test_trainer/checkpoint-125/config.json\n",
      "Model weights saved in test_trainer/checkpoint-125/model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in test_trainer/checkpoint-125/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/checkpoint-125/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-250\n",
      "Configuration saved in test_trainer/checkpoint-250/config.json\n",
      "Model weights saved in test_trainer/checkpoint-250/model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in test_trainer/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/checkpoint-250/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to final_model\n",
      "Configuration saved in final_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in final_model/model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in final_model/tokenizer_config.json\n",
      "Special tokens file saved in final_model/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'final_model' directory\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: {'eval_loss': 0.8708663582801819, 'eval_accuracy': 0.643, 'eval_runtime': 29.1096, 'eval_samples_per_second': 34.353, 'eval_steps_per_second': 4.294, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Optional: disable Weights & Biases logging\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "n_gpu = torch.cuda.device_count() or 1\n",
    "print(f\"Using {n_gpu} GPU(s)\")\n",
    "\n",
    "per_device_batch_size = 8  # Increased from 1\n",
    "grad_accumulation = 1\n",
    "total_batch_size = per_device_batch_size * n_gpu * grad_accumulation\n",
    "steps_per_epoch = max(1, len(small_train_dataset) // total_batch_size)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "# Compute metrics function using sklearn\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Flatten predictions and labels\n",
    "    flat_predictions = predictions.flatten()\n",
    "    flat_labels = np.asarray(labels).flatten()\n",
    "\n",
    "    # Filter out ignored loss indices (-100)\n",
    "    valid_indices = flat_labels != -100\n",
    "    valid_predictions = flat_predictions[valid_indices].astype(int)\n",
    "    valid_labels = flat_labels[valid_indices].astype(int)\n",
    "\n",
    "    # Compute accuracy\n",
    "    return {\"accuracy\": accuracy_score(valid_labels, valid_predictions)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    per_device_eval_batch_size=per_device_batch_size,\n",
    "    gradient_accumulation_steps=grad_accumulation,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-5,\n",
    "\n",
    "    # Use the correct parameter name\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=steps_per_epoch,\n",
    "\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize Data Collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(\"final_model\")\n",
    "print(\"Model saved to 'final_model' directory\")\n",
    "\n",
    "# Evaluate\n",
    "if small_eval_dataset is not None:\n",
    "    print(\"Evaluating on validation set...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Validation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_MtdOYykjRX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263,
     "referenced_widgets": [
      "5ec0d3f4850644e985952e57adc87dcb",
      "e94b09cd64484885bf92363424fd3afe",
      "6c2610888c07420f85b5618b7effba9c",
      "960f85b6080c4b7baa83c557b19ba894",
      "540b3e75a0484f06a98d6b1541f7c218",
      "ea02274f57504a72bb0b94de917d6b51",
      "03544da3de364796a091e74f6aa06d3e",
      "cc82872680234d688b11dbab53a37d79",
      "8658591ad71b462e94a2d8808cca603e",
      "fbf089332b7349f1b79b099a11f2a7be",
      "3ed6e5127aee4c31a32f00865d2d6f35",
      "a3049aec4f9646e0851976a7e21372e4",
      "7408469d022e4b889f0bba3bdea2baf5",
      "88270acc1a4e4470b78c9ee4e493072d",
      "6eba47f6c296408fa6ab1bee7dfc18b2",
      "271f378691ed41d2aeaa970a75e3df59",
      "2c1fa555c29a4729913d2b717dcbc6aa",
      "0e6695416ce04f7e8c7325e33e1dcf22",
      "a8e844d43dde4c83b96234d762664b14",
      "36926df4b4f3424ba68f7f58088cc078"
     ]
    },
    "id": "14y4UrPiYmxY",
    "outputId": "dc42c02a-8ddc-4bac-c67e-4cf700d64185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.8.3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec0d3f4850644e985952e57adc87dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ed6f8fc720cc4f7e905f8829e6707e43",
      "c85c97fb17a348f28b090cdf692500e4",
      "8efc1309dd3345a68f57608e20e352b8",
      "69bcec82c3944d6195ce31950566fd74",
      "b121780973144a1189d3539fce542647",
      "43d27c10fbc44966be51a40bd8ef6b21",
      "36a4ebb311164c0c999977c6607e51fc",
      "50fd7c74cb194438ab0ce48ee6a63c5d",
      "fc4ab6e2c6204c479efd65ba217bea1e",
      "c8b25e766b4f4a53b231d50a75251791",
      "7f22e4ade9194f2db08733780eade912",
      "a1ce396210684222ace76064ca4043ef",
      "45b3b513d1e740bc86ad80df4a13e23d",
      "36d98711c223443fb3caa65ba2ddbb40",
      "657b5e9dbc6d4a0491e03015d06bd5b1",
      "e479507680354e77bbc7af84eec6516b",
      "0b4d34852f8543f5805cdbe524d3281e",
      "055832e44a3549349275be5be68f908f",
      "dd4ff93aa82b44e08c3a080af1d1b296",
      "bd5eaa02589e4678b01f45833d2a9f87",
      "789b5dc2e8fb4e829cca0c2a6f7e6bff",
      "89579d0a2d314045a4115ceece37083e",
      "fb0ee731360d492eb42ce49db5eec473",
      "8ec755dd5e3643a1af07756182aec870",
      "12a74dc18e894b10856874c577243cfe",
      "0e5047138bc94ecaa8f8977fba2a0d44",
      "92f8e4b0d1e84ed6b53de4ad71d8bbeb",
      "023bcbf5c0594105992c5570283ecc40",
      "4386f7e48d2e4563a8365e71944c23a1",
      "f6fcc3b3456346329489216a0176b97a",
      "f713a15e2d0446328896155a9102cf1c",
      "acc5d555e7f744b0876e509e44f77801",
      "6f3c6d5562424316889a568c4d88fb3c",
      "c59b1a446a904902b291416bb0bb5d36",
      "93a56ffa260d485eb0221877954a91df",
      "45e94c5315334eff9f89f54548ff54b4",
      "3691362191f74272bc4b8cb1cced7315",
      "6a8b3df098c64a2a83358eb587e97aa9",
      "abf61b0c50ce4bcd884b8ac3f02b6549",
      "2fa1be505cea406789c22a6b18e6dd6e",
      "e73d70b11a09485bb0a371dd942128d1",
      "98f3042f6c2c499c8a63e204e3f8199d",
      "e442ead598f94e478b3658aced81e1f1",
      "1ff83fbb4ce542e68e2ab7035ff40634",
      "ff71f9687df3495cad18f1a1319041cd",
      "feb2f8a163814a13a71d314c61613f63",
      "22666302c5c54756b954f0ad9e867115",
      "71b6b9e4a3e641979b3aa0d88c716d09",
      "8058ca5b27384a7fb8aa2f1ef2b42a9b",
      "38338843a5a5480fa0ad64a9e407cecf",
      "9cf4e55c088745d0a0c9b9a2892ccc4f",
      "a68241891666414380fac68584706708",
      "9c809df7936d45bdb86bbb0fa38d6612",
      "e08a289308144f198072d73cf4d3e196",
      "ba88a83bce3f4da3850e34a9c8067645",
      "7edff6def56247c09cec05fe88fa5c71",
      "b47167f339e442b4bf50bd9174157f61",
      "285b178cfb874915a8a8cffc92ed601a",
      "9858764fbf3942edbb87be564970ebb5",
      "0c801b786976402e89289ff31f8b50ed",
      "ccb680bca625431ba537fcdd973c7999",
      "3338fdb7292d4498b007c4074a0aa174",
      "e10fc8281cf540a4add0c2ef06d0b920",
      "b54657621fd842fcab9205a864abd246"
     ]
    },
    "collapsed": true,
    "id": "vFQ9sVc_jXHg",
    "outputId": "876934c7-c4d8-4d3d-e163-7f2d1951655a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6f8fc720cc4f7e905f8829e6707e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file final_model/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.55.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file final_model/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"pad_token_id\": 50256\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at final_model were not used when initializing GPT2LMHeadModel: ['score.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at final_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Configuration saved in /tmp/tmp50rpenux/config.json\n",
      "Configuration saved in /tmp/tmp50rpenux/generation_config.json\n",
      "Model weights saved in /tmp/tmp50rpenux/model.safetensors\n",
      "Uploading the following files to SMungai098/my-finetuned-gpt2: config.json,generation_config.json,model.safetensors,README.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055832e44a3549349275be5be68f908f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4386f7e48d2e4563a8365e71944c23a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa1be505cea406789c22a6b18e6dd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmp50rpenux/model.safetensors    :   0%|          |  548kB /  498MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf4e55c088745d0a0c9b9a2892ccc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /tmp/tmpnwzp1alq/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpnwzp1alq/special_tokens_map.json\n",
      "Uploading the following files to SMungai098/my-finetuned-gpt2: vocab.json,tokenizer.json,README.md,merges.txt,tokenizer_config.json,special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SMungai098/my-finetuned-gpt2/commit/aea56b8c96d83659795baa711953cb74028af180', commit_message='Upload tokenizer', commit_description='', oid='aea56b8c96d83659795baa711953cb74028af180', pr_url=None, repo_url=RepoUrl('https://huggingface.co/SMungai098/my-finetuned-gpt2', endpoint='https://huggingface.co', repo_type='model', repo_id='SMungai098/my-finetuned-gpt2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # paste your HF write token when prompted\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"final_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"final_model\")\n",
    "\n",
    "model.push_to_hub(\"SMungai098/my-finetuned-gpt2\")\n",
    "tokenizer.push_to_hub(\"SMungai098/my-finetuned-gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "K7wFCE2H46fi",
    "outputId": "ec1123d5-9b7a-4617-ccfe-13309a186b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
      "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.41.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.10.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.7)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m596.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "1a025c3a83a743008ed438ef8ecbd3df",
       "pip_warning": {
        "packages": [
         "nvidia"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install torch transformers gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fW2iroZd1zVJ"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"mungaistephen066@gmail.com\"\n",
    "!git config --global user.name \"MungaiMwangi001\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xuKinuFz-pZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Access the token from environment variable\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mYwaigEL2JFY",
    "outputId": "84b4438d-94ca-49ad-9096-6643d06e3670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'my-finetuned-gpt2-app'...\n",
      "remote: Enumerating objects: 10, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 10 (delta 1), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (10/10), 3.04 KiB | 1.52 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf my-finetuned-gpt2-app  # Clean existing folder\n",
    "\n",
    "!git clone https://huggingface.co/spaces/SMungai098/my-finetuned-gpt2-app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQ7iZviB2RDB",
    "outputId": "c0710a52-49ce-4cac-b38c-672e26c0a003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/my-finetuned-gpt2-app/my-finetuned-gpt2-app\n"
     ]
    }
   ],
   "source": [
    "%cd my-finetuned-gpt2-app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oj5tHCK12nwS"
   },
   "outputs": [],
   "source": [
    "app_code = '''\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_name = \"SMungai098/my-finetuned-gpt2\"  # replace with your HF repo ID\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    # Tokenize input tweet\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "\n",
    "    # Map class IDs to labels (change if your labels differ)\n",
    "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    return label_map.get(predicted_class_id, \"Unknown\")\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(fn=predict_sentiment, inputs=\"text\", outputs=\"text\", title=\"Twitter Sentiment Analyzer\")\n",
    "\n",
    "demo.launch(share=True)\n",
    "'''\n",
    "\n",
    "# Save to app.py\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(app_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihDUkrVv3PDi",
    "outputId": "dd480efb-b707-4eb4-fc0a-c2dde5970dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import gradio as gr\n",
      "import torch\n",
      "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n",
      "\n",
      "# Load your fine-tuned model and tokenizer\n",
      "model_name = \"SMungai098/my-finetuned-gpt2\"  # replace with your HF repo ID\n",
      "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
      "model = GPT2ForSequenceClassification.from_pretrained(model_name)\n",
      "model.eval()\n",
      "\n",
      "def predict_sentiment(text):\n",
      "    # Tokenize input tweet\n",
      "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
      "    with torch.no_grad():\n",
      "        outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_class_id = logits.argmax().item()\n",
      "\n",
      "    # Map class IDs to labels (change if your labels differ)\n",
      "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
      "    return label_map.get(predicted_class_id, \"Unknown\")\n",
      "\n",
      "# Create Gradio interface\n",
      "demo = gr.Interface(fn=predict_sentiment, inputs=\"text\", outputs=\"text\", title=\"Twitter Sentiment Analyzer\")\n",
      "\n",
      "demo.launch(share=True)\n"
     ]
    }
   ],
   "source": [
    "!cat app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CQV6QoP3Tdx",
    "outputId": "29e71df5-c558-4685-bbb0-fcc078762168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mGPT2FTnv.ipynb\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!git add app.py\n",
    "!git commit -m \"Add greeting app\"\n",
    "!git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqC4VkpnKPJG",
    "outputId": "45be92e1-65c9-4829-ea32-fba3082bf71b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ku4EV1SEMmv9",
    "outputId": "4f83d43f-4a44-4eb1-8b4a-4e1a40f92fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Colab Notebooks'\n",
      "'The Dalton Grand Hotel is a newly opened upscale hotel located in the heart of Murang’a, Kenya, just a 3-minute walk from Rapids Camp.gdoc'\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grH65XWdNuOb",
    "outputId": "9840297a-3f8d-4755-dbc3-c02d43cafa02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2FT.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/drive/MyDrive/Colab Notebooks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipEMuz9zNxAI",
    "outputId": "27f69a2f-760a-4bb1-a11b-535efd70b205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Cleaned notebook saved to /content/drive/MyDrive/Colab Notebooks/GPT2FT_cleaned.ipynb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import nbformat\n",
    "\n",
    "notebook_path = '/content/drive/MyDrive/Colab Notebooks/GPT2FT.ipynb'\n",
    "\n",
    "nb = nbformat.read(notebook_path, as_version=nbformat.NO_CONVERT)\n",
    "\n",
    "if 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "cleaned_path = '/content/drive/MyDrive/Colab Notebooks/GPT2FT_cleaned.ipynb'\n",
    "nbformat.write(nb, cleaned_path)\n",
    "\n",
    "print(f\"Cleaned notebook saved to {cleaned_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
